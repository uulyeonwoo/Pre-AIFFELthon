{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fe60bd",
   "metadata": {
    "id": "hh8pZ7i9QK3a"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065a2ab2",
   "metadata": {
    "id": "37eacdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 경로 목록:\n",
      "/home/segmentsafestep/miniconda3/envs/jupyter-env/lib/python39.zip\n",
      "/home/segmentsafestep/miniconda3/envs/jupyter-env/lib/python3.9\n",
      "/home/segmentsafestep/miniconda3/envs/jupyter-env/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/segmentsafestep/miniconda3/envs/jupyter-env/lib/python3.9/site-packages\n",
      "/home/segmentsafestep/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/IPython/extensions\n",
      "/home/segmentsafestep/.ipython\n",
      "/content/drive/MyDrive/Colab Notebooks/AIFFELthon/bisenetv2_zh320\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules import conv3x3, conv1x1, DWConvBNAct, PWConvBNAct, ConvBNAct, Activation, SegHead\n",
    "from model_registry import register_model, aux_models\n",
    "\n",
    "# Python 모듈 경로에 'models' 폴더 추가\n",
    "models_path = \"/content/drive/MyDrive/Colab Notebooks/AIFFELthon/bisenetv2_zh320\"\n",
    "# utils_path = \"/aiffel/aiffel/aiffel project model training/Fast-SCNN-pytorch-master/utils\"\n",
    "\n",
    "if models_path not in sys.path:\n",
    "    sys.path.append(models_path)  # models 폴더 추가\n",
    "\n",
    "# sys.path 확인 (제대로 추가되었는지)\n",
    "print(\"Python 경로 목록:\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "\n",
    "train_img_path = \"/home/segmentsafestep/Fast-SCNN-pytorch-master/datasets/citys/leftImg8bit/train\"\n",
    "dataset_root = \"/home/segmentsafestep/Fast-SCNN-pytorch-master/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7f7e3",
   "metadata": {
    "id": "qVs3_abyQQxR"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7a9408",
   "metadata": {
    "id": "9a706466"
   },
   "outputs": [],
   "source": [
    "@register_model(aux_models)\n",
    "class BiSeNetv2(nn.Module):\n",
    "    def __init__(self, num_class=1, n_channel=3, act_type='relu', use_aux=True):\n",
    "        super().__init__()\n",
    "        self.use_aux = use_aux\n",
    "        self.detail_branch = DetailBranch(n_channel, 128, act_type)\n",
    "        self.semantic_branch = SemanticBranch(n_channel, 128, num_class, act_type, use_aux)\n",
    "        self.bga_layer = BilateralGuidedAggregationLayer(128, 128, act_type)\n",
    "        self.seg_head = SegHead(128, num_class, act_type)\n",
    "\n",
    "    def forward(self, x, is_training=False):\n",
    "        size = x.size()[2:]\n",
    "        x_d = self.detail_branch(x)\n",
    "        if self.use_aux:\n",
    "            x_s, aux2, aux3, aux4, aux5 = self.semantic_branch(x)\n",
    "        else:\n",
    "            x_s = self.semantic_branch(x)\n",
    "        x = self.bga_layer(x_d, x_s)\n",
    "        x = self.seg_head(x)\n",
    "        x = F.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        if self.use_aux and is_training:\n",
    "            return x, (aux2, aux3, aux4, aux5)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class DetailBranch(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, act_type='relu'):\n",
    "        super().__init__(\n",
    "            ConvBNAct(in_channels, 64, 3, 2, act_type=act_type),\n",
    "            ConvBNAct(64, 64, 3, 1, act_type=act_type),\n",
    "            ConvBNAct(64, 64, 3, 2, act_type=act_type),\n",
    "            ConvBNAct(64, 64, 3, 1, act_type=act_type),\n",
    "            ConvBNAct(64, 128, 3, 1, act_type=act_type),\n",
    "            ConvBNAct(128, 128, 3, 2, act_type=act_type),\n",
    "            ConvBNAct(128, 128, 3, 1, act_type=act_type),\n",
    "            ConvBNAct(128, out_channels, 3, 1, act_type=act_type)\n",
    "        )\n",
    "\n",
    "class SemanticBranch(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, num_class, act_type='relu', use_aux=False):\n",
    "        super().__init__()\n",
    "        self.use_aux = use_aux\n",
    "        self.stage1to2 = StemBlock(in_channels, 16, act_type)\n",
    "        self.stage3 = nn.Sequential(\n",
    "                            GatherExpansionLayer(16, 32, 2, act_type),\n",
    "                            GatherExpansionLayer(32, 32, 1, act_type),\n",
    "                        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "                            GatherExpansionLayer(32, 64, 2, act_type),\n",
    "                            GatherExpansionLayer(64, 64, 1, act_type),\n",
    "                        )\n",
    "        self.stage5_1to4 = nn.Sequential(\n",
    "                                GatherExpansionLayer(64, 128, 2, act_type),\n",
    "                                GatherExpansionLayer(128, 128, 1, act_type),\n",
    "                                GatherExpansionLayer(128, 128, 1, act_type),\n",
    "                                GatherExpansionLayer(128, 128, 1, act_type),\n",
    "                            )\n",
    "        self.stage5_5 = ContextEmbeddingBlock(128, out_channels, act_type)\n",
    "\n",
    "        if self.use_aux:\n",
    "            self.seg_head2 = SegHead(16, num_class, act_type)\n",
    "            self.seg_head3 = SegHead(32, num_class, act_type)\n",
    "            self.seg_head4 = SegHead(64, num_class, act_type)\n",
    "            self.seg_head5 = SegHead(128, num_class, act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1to2(x)\n",
    "        if self.use_aux:\n",
    "            aux2 = self.seg_head2(x)\n",
    "\n",
    "        x = self.stage3(x)\n",
    "        if self.use_aux:\n",
    "            aux3 = self.seg_head3(x)\n",
    "\n",
    "        x = self.stage4(x)\n",
    "        if self.use_aux:\n",
    "            aux4 = self.seg_head4(x)\n",
    "\n",
    "        x = self.stage5_1to4(x)\n",
    "        if self.use_aux:\n",
    "            aux5 = self.seg_head5(x)\n",
    "\n",
    "        x = self.stage5_5(x)\n",
    "\n",
    "        if self.use_aux:\n",
    "            return x, aux2, aux3, aux4, aux5\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class StemBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, act_type='relu'):\n",
    "        super().__init__()\n",
    "        self.conv_init = ConvBNAct(in_channels, out_channels, 3, 2, act_type=act_type)\n",
    "        self.left_branch = nn.Sequential(\n",
    "                            ConvBNAct(out_channels, out_channels//2, 1, act_type=act_type),\n",
    "                            ConvBNAct(out_channels//2, out_channels, 3, 2, act_type=act_type)\n",
    "                    )\n",
    "        self.right_branch = nn.MaxPool2d(3, 2, 1)\n",
    "        self.conv_last = ConvBNAct(out_channels*2, out_channels, 3, 1, act_type=act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_init(x)\n",
    "        x_left = self.left_branch(x)\n",
    "        x_right = self.right_branch(x)\n",
    "        x = torch.cat([x_left, x_right], dim=1)\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GatherExpansionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, act_type='relu', expand_ratio=6,):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        hid_channels = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        layers = [ConvBNAct(in_channels, in_channels, 3, act_type=act_type)]\n",
    "\n",
    "        if stride == 2:\n",
    "            layers.extend([\n",
    "                            DWConvBNAct(in_channels, hid_channels, 3, 2, act_type='none'),\n",
    "                            DWConvBNAct(hid_channels, hid_channels, 3, 1, act_type='none')\n",
    "                        ])\n",
    "            self.right_branch = nn.Sequential(\n",
    "                                    DWConvBNAct(in_channels, in_channels, 3, 2, act_type='none'),\n",
    "                                    PWConvBNAct(in_channels, out_channels, act_type='none')\n",
    "                            )\n",
    "        else:\n",
    "            layers.append(DWConvBNAct(in_channels, hid_channels, 3, 1, act_type='none'))\n",
    "\n",
    "        layers.append(PWConvBNAct(hid_channels, out_channels, act_type='none'))\n",
    "        self.left_branch = nn.Sequential(*layers)\n",
    "        self.act = Activation(act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.left_branch(x)\n",
    "\n",
    "        if self.stride == 2:\n",
    "            res = self.right_branch(x) + res\n",
    "        else:\n",
    "            res = x + res\n",
    "\n",
    "        return self.act(res)\n",
    "\n",
    "class ContextEmbeddingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, act_type='relu'):\n",
    "        super().__init__()\n",
    "        self.pool = nn.Sequential(\n",
    "                            nn.AdaptiveAvgPool2d(1),\n",
    "                            nn.BatchNorm2d(in_channels)\n",
    "                    )\n",
    "        self.conv_mid = ConvBNAct(in_channels, in_channels, 1, act_type=act_type)\n",
    "        self.conv_last = conv3x3(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.pool(x)\n",
    "        res = self.conv_mid(res)\n",
    "        x = res + x\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BilateralGuidedAggregationLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, act_type='relu'):\n",
    "        super().__init__()\n",
    "        self.detail_high = nn.Sequential(\n",
    "                                    DWConvBNAct(in_channels, in_channels, 3, act_type=act_type),\n",
    "                                    conv1x1(in_channels, in_channels)\n",
    "                        )\n",
    "        self.detail_low = nn.Sequential(\n",
    "                                    DWConvBNAct(in_channels, in_channels, 3, 2, act_type=act_type),\n",
    "                                    nn.AvgPool2d(3, 2, 1)\n",
    "                        )\n",
    "        self.semantic_high = nn.Sequential(\n",
    "                                    ConvBNAct(in_channels, in_channels, 3, act_type=act_type),\n",
    "                                    nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),\n",
    "                                    nn.Sigmoid()\n",
    "                            )\n",
    "        self.semantic_low = nn.Sequential(\n",
    "                                    DWConvBNAct(in_channels, in_channels, 3, act_type=act_type),\n",
    "                                    conv1x1(in_channels, in_channels),\n",
    "                                    nn.Sigmoid()\n",
    "                            )\n",
    "        self.conv_last = ConvBNAct(in_channels, out_channels, 3, act_type=act_type)\n",
    "\n",
    "    def forward(self, x_d, x_s):\n",
    "        x_d_high = self.detail_high(x_d)\n",
    "        x_d_low = self.detail_low(x_d)\n",
    "\n",
    "        x_s_high = self.semantic_high(x_s)\n",
    "        x_s_low = self.semantic_low(x_s)\n",
    "        x_high = x_d_high * x_s_high\n",
    "        x_low = x_d_low * x_s_low\n",
    "\n",
    "        size = x_high.size()[2:]\n",
    "        x_low = F.interpolate(x_low, size, mode='bilinear', align_corners=True)\n",
    "        res = x_high + x_low\n",
    "        res = self.conv_last(res)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16993c4f",
   "metadata": {
    "id": "e2a555d6"
   },
   "outputs": [],
   "source": [
    "model = BiSeNetv2(num_class=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95abc4f",
   "metadata": {
    "id": "3hwJecnkQWra"
   },
   "source": [
    "# Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7130c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1741536190074,
     "user": {
      "displayName": "Oochurl",
      "userId": "06588215150976747535"
     },
     "user_tz": -540
    },
    "id": "fc17548a",
    "outputId": "a65276ba-c3d6-48a6-c826-698055ccf4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detail_branch.0.0.weight --- torch.Size([64, 3, 3, 3])\n",
      "detail_branch.0.1.weight --- torch.Size([64])\n",
      "detail_branch.0.1.bias --- torch.Size([64])\n",
      "detail_branch.1.0.weight --- torch.Size([64, 64, 3, 3])\n",
      "detail_branch.1.1.weight --- torch.Size([64])\n",
      "detail_branch.1.1.bias --- torch.Size([64])\n",
      "detail_branch.2.0.weight --- torch.Size([64, 64, 3, 3])\n",
      "detail_branch.2.1.weight --- torch.Size([64])\n",
      "detail_branch.2.1.bias --- torch.Size([64])\n",
      "detail_branch.3.0.weight --- torch.Size([64, 64, 3, 3])\n",
      "detail_branch.3.1.weight --- torch.Size([64])\n",
      "detail_branch.3.1.bias --- torch.Size([64])\n",
      "detail_branch.4.0.weight --- torch.Size([128, 64, 3, 3])\n",
      "detail_branch.4.1.weight --- torch.Size([128])\n",
      "detail_branch.4.1.bias --- torch.Size([128])\n",
      "detail_branch.5.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "detail_branch.5.1.weight --- torch.Size([128])\n",
      "detail_branch.5.1.bias --- torch.Size([128])\n",
      "detail_branch.6.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "detail_branch.6.1.weight --- torch.Size([128])\n",
      "detail_branch.6.1.bias --- torch.Size([128])\n",
      "detail_branch.7.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "detail_branch.7.1.weight --- torch.Size([128])\n",
      "detail_branch.7.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage1to2.conv_init.0.weight --- torch.Size([16, 3, 3, 3])\n",
      "semantic_branch.stage1to2.conv_init.1.weight --- torch.Size([16])\n",
      "semantic_branch.stage1to2.conv_init.1.bias --- torch.Size([16])\n",
      "semantic_branch.stage1to2.left_branch.0.0.weight --- torch.Size([8, 16, 1, 1])\n",
      "semantic_branch.stage1to2.left_branch.0.1.weight --- torch.Size([8])\n",
      "semantic_branch.stage1to2.left_branch.0.1.bias --- torch.Size([8])\n",
      "semantic_branch.stage1to2.left_branch.1.0.weight --- torch.Size([16, 8, 3, 3])\n",
      "semantic_branch.stage1to2.left_branch.1.1.weight --- torch.Size([16])\n",
      "semantic_branch.stage1to2.left_branch.1.1.bias --- torch.Size([16])\n",
      "semantic_branch.stage1to2.conv_last.0.weight --- torch.Size([16, 32, 3, 3])\n",
      "semantic_branch.stage1to2.conv_last.1.weight --- torch.Size([16])\n",
      "semantic_branch.stage1to2.conv_last.1.bias --- torch.Size([16])\n",
      "semantic_branch.stage3.0.right_branch.0.0.weight --- torch.Size([16, 1, 3, 3])\n",
      "semantic_branch.stage3.0.right_branch.0.1.weight --- torch.Size([16])\n",
      "semantic_branch.stage3.0.right_branch.0.1.bias --- torch.Size([16])\n",
      "semantic_branch.stage3.0.right_branch.1.0.weight --- torch.Size([32, 16, 1, 1])\n",
      "semantic_branch.stage3.0.right_branch.1.0.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.0.right_branch.1.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage3.0.right_branch.1.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.0.left_branch.0.0.weight --- torch.Size([16, 16, 3, 3])\n",
      "semantic_branch.stage3.0.left_branch.0.1.weight --- torch.Size([16])\n",
      "semantic_branch.stage3.0.left_branch.0.1.bias --- torch.Size([16])\n",
      "semantic_branch.stage3.0.left_branch.1.0.weight --- torch.Size([96, 1, 3, 3])\n",
      "semantic_branch.stage3.0.left_branch.1.1.weight --- torch.Size([96])\n",
      "semantic_branch.stage3.0.left_branch.1.1.bias --- torch.Size([96])\n",
      "semantic_branch.stage3.0.left_branch.2.0.weight --- torch.Size([96, 1, 3, 3])\n",
      "semantic_branch.stage3.0.left_branch.2.1.weight --- torch.Size([96])\n",
      "semantic_branch.stage3.0.left_branch.2.1.bias --- torch.Size([96])\n",
      "semantic_branch.stage3.0.left_branch.3.0.weight --- torch.Size([32, 96, 1, 1])\n",
      "semantic_branch.stage3.0.left_branch.3.0.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.0.left_branch.3.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage3.0.left_branch.3.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.1.left_branch.0.0.weight --- torch.Size([32, 32, 3, 3])\n",
      "semantic_branch.stage3.1.left_branch.0.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage3.1.left_branch.0.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.1.left_branch.1.0.weight --- torch.Size([192, 1, 3, 3])\n",
      "semantic_branch.stage3.1.left_branch.1.1.weight --- torch.Size([192])\n",
      "semantic_branch.stage3.1.left_branch.1.1.bias --- torch.Size([192])\n",
      "semantic_branch.stage3.1.left_branch.2.0.weight --- torch.Size([32, 192, 1, 1])\n",
      "semantic_branch.stage3.1.left_branch.2.0.bias --- torch.Size([32])\n",
      "semantic_branch.stage3.1.left_branch.2.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage3.1.left_branch.2.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage4.0.right_branch.0.0.weight --- torch.Size([32, 1, 3, 3])\n",
      "semantic_branch.stage4.0.right_branch.0.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage4.0.right_branch.0.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage4.0.right_branch.1.0.weight --- torch.Size([64, 32, 1, 1])\n",
      "semantic_branch.stage4.0.right_branch.1.0.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.0.right_branch.1.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage4.0.right_branch.1.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.0.left_branch.0.0.weight --- torch.Size([32, 32, 3, 3])\n",
      "semantic_branch.stage4.0.left_branch.0.1.weight --- torch.Size([32])\n",
      "semantic_branch.stage4.0.left_branch.0.1.bias --- torch.Size([32])\n",
      "semantic_branch.stage4.0.left_branch.1.0.weight --- torch.Size([192, 1, 3, 3])\n",
      "semantic_branch.stage4.0.left_branch.1.1.weight --- torch.Size([192])\n",
      "semantic_branch.stage4.0.left_branch.1.1.bias --- torch.Size([192])\n",
      "semantic_branch.stage4.0.left_branch.2.0.weight --- torch.Size([192, 1, 3, 3])\n",
      "semantic_branch.stage4.0.left_branch.2.1.weight --- torch.Size([192])\n",
      "semantic_branch.stage4.0.left_branch.2.1.bias --- torch.Size([192])\n",
      "semantic_branch.stage4.0.left_branch.3.0.weight --- torch.Size([64, 192, 1, 1])\n",
      "semantic_branch.stage4.0.left_branch.3.0.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.0.left_branch.3.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage4.0.left_branch.3.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.1.left_branch.0.0.weight --- torch.Size([64, 64, 3, 3])\n",
      "semantic_branch.stage4.1.left_branch.0.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage4.1.left_branch.0.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.1.left_branch.1.0.weight --- torch.Size([384, 1, 3, 3])\n",
      "semantic_branch.stage4.1.left_branch.1.1.weight --- torch.Size([384])\n",
      "semantic_branch.stage4.1.left_branch.1.1.bias --- torch.Size([384])\n",
      "semantic_branch.stage4.1.left_branch.2.0.weight --- torch.Size([64, 384, 1, 1])\n",
      "semantic_branch.stage4.1.left_branch.2.0.bias --- torch.Size([64])\n",
      "semantic_branch.stage4.1.left_branch.2.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage4.1.left_branch.2.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage5_1to4.0.right_branch.0.0.weight --- torch.Size([64, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.0.right_branch.0.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage5_1to4.0.right_branch.0.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage5_1to4.0.right_branch.1.0.weight --- torch.Size([128, 64, 1, 1])\n",
      "semantic_branch.stage5_1to4.0.right_branch.1.0.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.0.right_branch.1.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.0.right_branch.1.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.0.left_branch.0.0.weight --- torch.Size([64, 64, 3, 3])\n",
      "semantic_branch.stage5_1to4.0.left_branch.0.1.weight --- torch.Size([64])\n",
      "semantic_branch.stage5_1to4.0.left_branch.0.1.bias --- torch.Size([64])\n",
      "semantic_branch.stage5_1to4.0.left_branch.1.0.weight --- torch.Size([384, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.0.left_branch.1.1.weight --- torch.Size([384])\n",
      "semantic_branch.stage5_1to4.0.left_branch.1.1.bias --- torch.Size([384])\n",
      "semantic_branch.stage5_1to4.0.left_branch.2.0.weight --- torch.Size([384, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.0.left_branch.2.1.weight --- torch.Size([384])\n",
      "semantic_branch.stage5_1to4.0.left_branch.2.1.bias --- torch.Size([384])\n",
      "semantic_branch.stage5_1to4.0.left_branch.3.0.weight --- torch.Size([128, 384, 1, 1])\n",
      "semantic_branch.stage5_1to4.0.left_branch.3.0.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.0.left_branch.3.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.0.left_branch.3.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.1.left_branch.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "semantic_branch.stage5_1to4.1.left_branch.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.1.left_branch.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.1.left_branch.1.0.weight --- torch.Size([768, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.1.left_branch.1.1.weight --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.1.left_branch.1.1.bias --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.1.left_branch.2.0.weight --- torch.Size([128, 768, 1, 1])\n",
      "semantic_branch.stage5_1to4.1.left_branch.2.0.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.1.left_branch.2.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.1.left_branch.2.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.2.left_branch.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "semantic_branch.stage5_1to4.2.left_branch.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.2.left_branch.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.2.left_branch.1.0.weight --- torch.Size([768, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.2.left_branch.1.1.weight --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.2.left_branch.1.1.bias --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.2.left_branch.2.0.weight --- torch.Size([128, 768, 1, 1])\n",
      "semantic_branch.stage5_1to4.2.left_branch.2.0.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.2.left_branch.2.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.2.left_branch.2.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.3.left_branch.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "semantic_branch.stage5_1to4.3.left_branch.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.3.left_branch.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.3.left_branch.1.0.weight --- torch.Size([768, 1, 3, 3])\n",
      "semantic_branch.stage5_1to4.3.left_branch.1.1.weight --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.3.left_branch.1.1.bias --- torch.Size([768])\n",
      "semantic_branch.stage5_1to4.3.left_branch.2.0.weight --- torch.Size([128, 768, 1, 1])\n",
      "semantic_branch.stage5_1to4.3.left_branch.2.0.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.3.left_branch.2.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_1to4.3.left_branch.2.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_5.pool.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_5.pool.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_5.conv_mid.0.weight --- torch.Size([128, 128, 1, 1])\n",
      "semantic_branch.stage5_5.conv_mid.1.weight --- torch.Size([128])\n",
      "semantic_branch.stage5_5.conv_mid.1.bias --- torch.Size([128])\n",
      "semantic_branch.stage5_5.conv_last.weight --- torch.Size([128, 128, 3, 3])\n",
      "semantic_branch.seg_head2.0.0.weight --- torch.Size([128, 16, 3, 3])\n",
      "semantic_branch.seg_head2.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.seg_head2.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.seg_head2.1.weight --- torch.Size([19, 128, 1, 1])\n",
      "semantic_branch.seg_head3.0.0.weight --- torch.Size([128, 32, 3, 3])\n",
      "semantic_branch.seg_head3.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.seg_head3.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.seg_head3.1.weight --- torch.Size([19, 128, 1, 1])\n",
      "semantic_branch.seg_head4.0.0.weight --- torch.Size([128, 64, 3, 3])\n",
      "semantic_branch.seg_head4.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.seg_head4.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.seg_head4.1.weight --- torch.Size([19, 128, 1, 1])\n",
      "semantic_branch.seg_head5.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "semantic_branch.seg_head5.0.1.weight --- torch.Size([128])\n",
      "semantic_branch.seg_head5.0.1.bias --- torch.Size([128])\n",
      "semantic_branch.seg_head5.1.weight --- torch.Size([19, 128, 1, 1])\n",
      "bga_layer.detail_high.0.0.weight --- torch.Size([128, 1, 3, 3])\n",
      "bga_layer.detail_high.0.1.weight --- torch.Size([128])\n",
      "bga_layer.detail_high.0.1.bias --- torch.Size([128])\n",
      "bga_layer.detail_high.1.weight --- torch.Size([128, 128, 1, 1])\n",
      "bga_layer.detail_low.0.0.weight --- torch.Size([128, 1, 3, 3])\n",
      "bga_layer.detail_low.0.1.weight --- torch.Size([128])\n",
      "bga_layer.detail_low.0.1.bias --- torch.Size([128])\n",
      "bga_layer.semantic_high.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "bga_layer.semantic_high.0.1.weight --- torch.Size([128])\n",
      "bga_layer.semantic_high.0.1.bias --- torch.Size([128])\n",
      "bga_layer.semantic_low.0.0.weight --- torch.Size([128, 1, 3, 3])\n",
      "bga_layer.semantic_low.0.1.weight --- torch.Size([128])\n",
      "bga_layer.semantic_low.0.1.bias --- torch.Size([128])\n",
      "bga_layer.semantic_low.1.weight --- torch.Size([128, 128, 1, 1])\n",
      "bga_layer.conv_last.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "bga_layer.conv_last.1.weight --- torch.Size([128])\n",
      "bga_layer.conv_last.1.bias --- torch.Size([128])\n",
      "seg_head.0.0.weight --- torch.Size([128, 128, 3, 3])\n",
      "seg_head.0.1.weight --- torch.Size([128])\n",
      "seg_head.0.1.bias --- torch.Size([128])\n",
      "seg_head.1.weight --- torch.Size([19, 128, 1, 1])\n",
      "tot_num_param:  2559920\n"
     ]
    }
   ],
   "source": [
    "chk_pnt_path = \"/home/segmentsafestep/Fast-SCNN-pytorch-master/weights/bisenetv2-aux.pth\"\n",
    "chk_pnt_map = torch.load(chk_pnt_path, map_location='cpu')\n",
    "\n",
    "load_res = model.load_state_dict(chk_pnt_map[\"state_dict\"], strict=False)\n",
    "\n",
    "# 가중치가 잘 로드 되었는지 확인\n",
    "load_res.missing_keys, load_res.unexpected_keys\n",
    "\n",
    "# 파라미터 개수, 레이어 확인\n",
    "tot_num_param = 0\n",
    "for layer_name, params in model.named_parameters():\n",
    "    print(layer_name,'---', params.shape)\n",
    "    n_p = 1\n",
    "    for p in params.shape:\n",
    "        n_p *= p\n",
    "    tot_num_param += n_p\n",
    "\n",
    "print(\"tot_num_param: \", tot_num_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57def4bd",
   "metadata": {
    "id": "xFBvK2W1XZv2"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474d4677",
   "metadata": {
    "id": "da3def4d"
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# PyTorch 관련 라이브러리\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 이미지 변환 관련 라이브러리\n",
    "from torchvision import transforms\n",
    "\n",
    "# 데이터 로더 및 모델, 유틸리티 함수 불러오기\n",
    "from data_loader import get_segmentation_dataset  # 데이터셋 불러오는 함수\n",
    "from utils.loss import MixSoftmaxCrossEntropyLoss, MixSoftmaxCrossEntropyOHEMLoss  # 손실 함수\n",
    "from utils.lr_scheduler import LRScheduler  # 학습률 스케줄러\n",
    "from utils.metric import SegmentationMetric  # 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba8d9c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1741536141025,
     "user": {
      "displayName": "Oochurl",
      "userId": "06588215150976747535"
     },
     "user_tz": -540
    },
    "id": "eb2a7021",
    "outputId": "28c097fb-7ff2-46b5-f2b8-1df1a604811b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 학습 관련 하이퍼파라미터 설정\n",
    "args = {\n",
    "    \"model\": \"BiSeNetv2\",  # 사용할 모델 이름\n",
    "    \"dataset\": \"citys\",  # 학습할 데이터셋 (예: Cityscapes)\n",
    "    \"base_size\": 1024,  # 입력 이미지 기본 크기\n",
    "    \"crop_size\": 768,  # 학습 시 사용할 크롭 크기\n",
    "    \"train_split\": \"train\",  # 학습 데이터셋의 분할 방식\n",
    "\n",
    "    # 학습 하이퍼파라미터\n",
    "    \"aux\": False,  # 보조 손실 사용 여부\n",
    "    \"aux_weight\": 0.4,  # 보조 손실 가중치\n",
    "    \"epochs\": 1,  # 학습 에폭 수\n",
    "    \"start_epoch\": 0,  # 학습 시작 에폭\n",
    "    \"batch_size\": 2,  # 배치 크기\n",
    "    \"lr\": 1e-2,  # 학습률\n",
    "    \"momentum\": 0.9,  # 모멘텀\n",
    "    \"weight_decay\": 1e-4,  # 가중치 감쇠 (L2 정규화)\n",
    "\n",
    "    # 체크포인트 저장 위치\n",
    "    \"resume\": None,  # 기존 모델 체크포인트 (사용하지 않음)\n",
    "    \"save_folder\": \"./weights\",  # 모델 가중치 저장 경로\n",
    "\n",
    "    # 평가 및 검증 설정\n",
    "    \"eval\": False,  # 평가 모드 여부\n",
    "    \"no_val\": True,  # 검증 생략 여부\n",
    "}\n",
    "\n",
    "# GPU 사용 여부 설정\n",
    "print(device := torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "cudnn.benchmark = True  # GPU 연산 최적화\n",
    "args[\"device\"] = device  # 학습에 사용할 디바이스 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9540eb09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1741536141085,
     "user": {
      "displayName": "Oochurl",
      "userId": "06588215150976747535"
     },
     "user_tz": -540
    },
    "id": "e1d92fca",
    "outputId": "9067dcd7-1398-4572-e49f-f794584e526d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 images in /home/segmentsafestep/Fast-SCNN-pytorch-master/datasets/citys/leftImg8bit/train\n",
      "Found 500 images in /home/segmentsafestep/Fast-SCNN-pytorch-master/datasets/citys/leftImg8bit/val\n"
     ]
    }
   ],
   "source": [
    "# 이미지 전처리 변환 설정\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환\n",
    "    # transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "    transforms.Normalize([0.3257, 0.3690, 0.3223], [0.2112, 0.2148, 0.2115]),  # 이미지 정규화\n",
    "])\n",
    "# mean=(0.3257, 0.3690, 0.3223), # city, rgb\n",
    "# std=(0.2112, 0.2148, 0.2115),\n",
    "\n",
    "# 데이터셋 로드 (학습 및 검증)\n",
    "data_kwargs = {\"transform\": input_transform, \"base_size\": args[\"base_size\"], \"crop_size\": args[\"crop_size\"]}\n",
    "train_dataset = get_segmentation_dataset(args[\"dataset\"], split=args[\"train_split\"], mode=\"train\", root=dataset_root, **data_kwargs)\n",
    "val_dataset = get_segmentation_dataset(args[\"dataset\"], split=\"val\", mode=\"val\", root=dataset_root, **data_kwargs)\n",
    "\n",
    "# DataLoader 생성 (데이터 배치 단위로 로딩)\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=args[\"batch_size\"], shuffle=True, num_workers=4 # 이거 수정해봐\n",
    "                               , drop_last=True)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 모델을 GPU 또는 CPU로 이동\n",
    "dev_mod = model.to(args[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0578754",
   "metadata": {
    "id": "b1566b5a"
   },
   "outputs": [],
   "source": [
    "# 손실 함수 설정 (OHEM Loss 사용)\n",
    "# criterion = MixSoftmaxCrossEntropyOHEMLoss(aux=args[\"aux\"], aux_weight=args[\"aux_weight\"], ignore_index=-1).to(args[\"device\"])\n",
    "criterion = MixSoftmaxCrossEntropyLoss(aux=args[\"aux\"], aux_weight=args[\"aux_weight\"], ignore_label=-1).to(args[\"device\"])\n",
    "\n",
    "# 옵티마이저 (SGD 사용)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args[\"lr\"], momentum=args[\"momentum\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "# 학습률 스케줄러 설정 (poly decay 사용)\n",
    "lr_scheduler = LRScheduler(mode=\"poly\", base_lr=args[\"lr\"], nepochs=args[\"epochs\"],\n",
    "                           iters_per_epoch=len(train_loader), power=0.9)\n",
    "\n",
    "# 평가 지표 (mIoU 등)\n",
    "metric = SegmentationMetric(train_dataset.num_class)\n",
    "\n",
    "# 최고 성능 저장을 위한 변수\n",
    "best_pred = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac0aaa6",
   "metadata": {
    "id": "e41cceb6"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    global best_pred  # 최고 성능 비교를 위한 전역 변수 사용\n",
    "    cur_iters = 0  # 현재 반복 횟수\n",
    "    start_time = time.time()  # 학습 시작 시간\n",
    "\n",
    "    # 각 에폭마다 학습 수행\n",
    "    for epoch in range(args[\"start_epoch\"], args[\"epochs\"]):\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            cur_lr = lr_scheduler(cur_iters)  # 현재 학습률 설정\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = cur_lr  # 옵티마이저에 학습률 적용\n",
    "\n",
    "            # 데이터를 GPU/CPU로 이동\n",
    "            images, targets = images.to(args[\"device\"]), targets.to(args[\"device\"])\n",
    "\n",
    "            # 모델 예측 및 손실 계산\n",
    "            outputs = model(images)\n",
    "            # outputs = outputs.argmax(dim=1) # BiSeNetV2_zh320\n",
    "            loss = criterion((outputs,), targets)\n",
    "\n",
    "            # 역전파 및 최적화 수행\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cur_iters += 1  # 반복 횟수 증가\n",
    "            if cur_iters % 200 == 0:  # 10회마다 로그 출력\n",
    "                print(f\"Epoch [{epoch}/{args['epochs']}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        end_time = time.time()  # 에폭 종료 시간 기록\n",
    "        epoch_time = end_time - start_time  # 소요 시간 계산\n",
    "        print(f\"Epoch {epoch} 완료! 소요 시간: {epoch_time:.2f} 초\")\n",
    "\n",
    "        # 5의 배수 에폭일 때만 저장\n",
    "        if epoch % 5 == 0:\n",
    "            save_path = os.path.join(args[\"save_folder\"], f\"model_19class_mioutest{epoch}.pth\")\n",
    "            torch.save({\"epoch\": epoch, \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"cur_iters\": cur_iters}, save_path)\n",
    "            print(f\"모델 가중치 저장됨: {save_path}\")\n",
    "\n",
    "        # 검증 수행 및 체크포인트 저장\n",
    "        if not args[\"no_val\"]:\n",
    "            validation(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ce0c1f",
   "metadata": {
    "id": "40e0fdcd"
   },
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    global best_pred  # 최고 성능 비교를 위한 전역 변수 사용\n",
    "    model.eval()  # 모델을 평가 모드로 변경\n",
    "    metric.reset()  # 평가 지표 초기화\n",
    "\n",
    "    for i, (image, target) in enumerate(val_loader):\n",
    "        image = image.to(args[\"device\"])  # 이미지를 GPU/CPU로 이동\n",
    "\n",
    "        with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "            outputs = (model(image),)\n",
    "\n",
    "        pred = torch.argmax(outputs[0], 1).cpu().data.numpy()  # 예측 결과 가져오기\n",
    "        metric.update(pred, target.numpy())  # 평가 지표 업데이트\n",
    "\n",
    "    pixAcc, mIoU = metric.get()  # 픽셀 정확도 및 mIoU 계산\n",
    "    print(f\"Epoch {epoch}, Validation PixAcc: {pixAcc:.3f}, mIoU: {mIoU:.3f}\")\n",
    "\n",
    "    new_pred = (pixAcc + mIoU) / 2  # 성능 평가\n",
    "    if new_pred > best_pred:\n",
    "        best_pred = new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ea1499",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2647,
     "status": "error",
     "timestamp": 1741536144129,
     "user": {
      "displayName": "Oochurl",
      "userId": "06588215150976747535"
     },
     "user_tz": -540
    },
    "id": "FBqVRmTKbLES",
    "outputId": "c36f58a7-1521-4778-d26f-73e763f61275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], Step [199/1487], Loss: 1.0644\n",
      "Epoch [0/1], Step [399/1487], Loss: 0.6966\n",
      "Epoch [0/1], Step [599/1487], Loss: 2.0011\n",
      "Epoch [0/1], Step [799/1487], Loss: 1.2523\n",
      "Epoch [0/1], Step [999/1487], Loss: 0.8930\n",
      "Epoch [0/1], Step [1199/1487], Loss: 0.7980\n",
      "Epoch [0/1], Step [1399/1487], Loss: 0.8974\n",
      "Epoch 0 완료! 소요 시간: 226.88 초\n",
      "모델 가중치 저장됨: ./weights/model_19class_mioutest0.pth\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb65e3",
   "metadata": {
    "id": "91Gj6Q732NyZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeee19ea-799d-4d90-a3a8-f1391c4166fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from data_loader import get_segmentation_dataset  # 데이터셋 로드 함수\n",
    "from utils.metric import SegmentationMetric  # 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9418347-4c83-4e98-b90a-078bd699d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images in /home/segmentsafestep/Fast-SCNN-pytorch-master/datasets/citys/leftImg8bit/val\n",
      "모델 및 가중치 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 설정 (Jupyter Notebook 환경에 맞게 변경)\n",
    "args = {\n",
    "    \"dataset\": \"citys\",  # 사용할 데이터셋 (Cityscapes)\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),  # GPU/CPU 설정\n",
    "    \"batch_size\": 1,  # 평가 시 배치 크기\n",
    "    \"num_workers\": 0,  # DataLoader의 worker 개수 (메모리 문제 방지 위해 0으로 설정)\n",
    "    \"weight_path\": \"/home/segmentsafestep/Fast-SCNN-pytorch-master/weights/model_19class_mioutest0.pth\"  # 저장된 가중치 경로\n",
    "}\n",
    "\n",
    "# 가중치 파일 확인\n",
    "if not os.path.exists(args[\"weight_path\"]):\n",
    "    raise FileNotFoundError(f\"가중치 파일이 존재하지 않습니다: {args['weight_path']}\")\n",
    "\n",
    "# 이미지 전처리 설정\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])\n",
    "\n",
    "# 검증 데이터셋 로드 (Cityscapes val set)\n",
    "val_dataset = get_segmentation_dataset(args[\"dataset\"], split=\"val\", mode=\"testval\", transform=input_transform, root=\"/home/segmentsafestep/Fast-SCNN-pytorch-master/datasets\")\n",
    "val_loader = data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=args[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=args[\"num_workers\"]\n",
    ")\n",
    "\n",
    "# 모델 및 가중치 로드\n",
    "# model = BiSeNetv2(num_class=val_dataset.num_class).to(args[\"device\"])  # 모델 생성\n",
    "\n",
    "# 저장된 가중치 파일 로드\n",
    "checkpoint = torch.load(args[\"weight_path\"], map_location=args[\"device\"])\n",
    "\n",
    "# 가중치 파일에 state_dict가 포함되어 있으면, 모델 가중치만 추출하여 로드\n",
    "if \"state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])  # state_dict만 로드\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)  # 이미 순수한 가중치 파일이면 그대로 로드\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "print(\"모델 및 가중치 로드 완료!\")\n",
    "\n",
    "# 평가 지표 설정 (mIoU 및 픽셀 정확도)\n",
    "metric = SegmentationMetric(val_dataset.num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21fad2b9-8fee-4be7-9ab5-d9b547fdcc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터셋 평가 중...\n",
      "Sample 10: Pixel Accuracy = 67.37%, mIoU = 16.02%\n",
      "Sample 20: Pixel Accuracy = 65.69%, mIoU = 15.06%\n",
      "Sample 30: Pixel Accuracy = 63.89%, mIoU = 15.34%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4197/2030583080.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"검증 데이터셋 평가 중...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# GPU -> CPU 후 불필요한 차원 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Fast-SCNN-pytorch-master/data_loader/cityscapes.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter-env/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 검증 수행 및 mIoU 계산\n",
    "print(\"검증 데이터셋 평가 중...\")\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(val_loader):\n",
    "        image = image.to(args[\"device\"])\n",
    "        label = label.cpu().numpy().squeeze()  # GPU -> CPU 후 불필요한 차원 제거\n",
    "\n",
    "        # 모델 예측 수행\n",
    "        outputs = model(image)\n",
    "\n",
    "        # 모델 출력이 (C, W)로 잘못 나오는 경우, 올바르게 변환\n",
    "        if len(outputs.shape) == 3:  # (C, H, W) 형태라면 batch 차원이 없음 -> 추가\n",
    "            outputs = outputs.unsqueeze(0)  # (1, C, H, W) 형태로 변환\n",
    "        elif len(outputs.shape) == 2:  # (C, W) 형태라면 차원 재배열 필요\n",
    "            outputs = outputs.view(1, outputs.shape[0], 1, outputs.shape[1])  # (1, C, 1, W)로 변환\n",
    "        \n",
    "        # 예측 결과 계산\n",
    "        pred = torch.argmax(outputs, dim=1).cpu().numpy().squeeze()  # (H, W) 형태로 변환\n",
    "        \n",
    "        # 평가 지표 업데이트\n",
    "        metric.update(pred, label)\n",
    "\n",
    "        # 10개마다 중간 결과 출력\n",
    "        if (i + 1) % 10 == 0:\n",
    "            pixAcc, mIoU = metric.get()\n",
    "            print(f\"Sample {i+1}: Pixel Accuracy = {pixAcc * 100:.2f}%, mIoU = {mIoU * 100:.2f}%\")\n",
    "\n",
    "# 최종 평가 결과 출력\n",
    "final_pixAcc, final_mIoU = metric.get()\n",
    "print(\"\\n최종 평가 결과\")\n",
    "print(f\"Pixel Accuracy: {final_pixAcc * 100:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {final_mIoU * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd2b18-b520-44de-8f55-924540519ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hh8pZ7i9QK3a",
    "qVs3_abyQQxR",
    "3hwJecnkQWra"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
